{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the libraries you will need\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    " \n",
    "from sklearn.linear_model import perceptron\n",
    "from pandas import *\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    " \n",
    "# You only need this if using Notebook\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>having_IP_Address</th>\n",
       "      <th>URL_Length</th>\n",
       "      <th>Shortining_Service</th>\n",
       "      <th>having_At_Symbol</th>\n",
       "      <th>double_slash_redirecting</th>\n",
       "      <th>Prefix_Suffix</th>\n",
       "      <th>having_Sub_Domain</th>\n",
       "      <th>SSLfinal_State</th>\n",
       "      <th>Domain_registeration_length</th>\n",
       "      <th>Favicon</th>\n",
       "      <th>...</th>\n",
       "      <th>popUpWidnow</th>\n",
       "      <th>Iframe</th>\n",
       "      <th>age_of_domain</th>\n",
       "      <th>DNSRecord</th>\n",
       "      <th>web_traffic</th>\n",
       "      <th>Page_Rank</th>\n",
       "      <th>Google_Index</th>\n",
       "      <th>Links_pointing_to_page</th>\n",
       "      <th>Statistical_report</th>\n",
       "      <th>Result</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11050</th>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11051</th>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>...</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11052</th>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11053</th>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>...</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11054</th>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       having_IP_Address  URL_Length  Shortining_Service  having_At_Symbol  \\\n",
       "11050                  1          -1                   1                -1   \n",
       "11051                 -1           1                   1                -1   \n",
       "11052                  1          -1                   1                 1   \n",
       "11053                 -1          -1                   1                 1   \n",
       "11054                 -1          -1                   1                 1   \n",
       "\n",
       "       double_slash_redirecting  Prefix_Suffix  having_Sub_Domain  \\\n",
       "11050                         1              1                  1   \n",
       "11051                        -1             -1                  1   \n",
       "11052                         1             -1                  1   \n",
       "11053                         1             -1                 -1   \n",
       "11054                         1             -1                 -1   \n",
       "\n",
       "       SSLfinal_State  Domain_registeration_length  Favicon   ...    \\\n",
       "11050               1                           -1       -1   ...     \n",
       "11051              -1                           -1       -1   ...     \n",
       "11052              -1                           -1        1   ...     \n",
       "11053              -1                            1       -1   ...     \n",
       "11054              -1                            1        1   ...     \n",
       "\n",
       "       popUpWidnow  Iframe  age_of_domain  DNSRecord  web_traffic  Page_Rank  \\\n",
       "11050           -1      -1              1          1           -1         -1   \n",
       "11051           -1       1              1          1            1          1   \n",
       "11052            1       1              1          1            1         -1   \n",
       "11053           -1       1              1          1            1         -1   \n",
       "11054            1       1             -1          1           -1         -1   \n",
       "\n",
       "       Google_Index  Links_pointing_to_page  Statistical_report  Result  \n",
       "11050             1                       1                   1       1  \n",
       "11051             1                      -1                   1      -1  \n",
       "11052             1                       0                   1      -1  \n",
       "11053             1                       1                   1      -1  \n",
       "11054            -1                       1                  -1      -1  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = pandas.read_csv('./Training_Data.csv')\n",
    "inputs_train = inputs[:-3316]\n",
    "inputs_test = inputs[-3316:]\n",
    "inputs.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch 1\n",
      "Norm: 0.23, NNZs: 30, Bias: 0.045000, T: 7739, Avg. loss: 0.006956\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 0.24, NNZs: 30, Bias: 0.065000, T: 15478, Avg. loss: 0.006993\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 0.23, NNZs: 30, Bias: 0.085000, T: 23217, Avg. loss: 0.006912\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 0.24, NNZs: 30, Bias: 0.080000, T: 30956, Avg. loss: 0.006934\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 0.26, NNZs: 30, Bias: 0.105000, T: 38695, Avg. loss: 0.006766\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 0.24, NNZs: 30, Bias: 0.105000, T: 46434, Avg. loss: 0.007080\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 0.24, NNZs: 30, Bias: 0.110000, T: 54173, Avg. loss: 0.007176\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 0.24, NNZs: 30, Bias: 0.120000, T: 61912, Avg. loss: 0.007134\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 0.24, NNZs: 30, Bias: 0.125000, T: 69651, Avg. loss: 0.006997\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 0.24, NNZs: 30, Bias: 0.120000, T: 77390, Avg. loss: 0.007153\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 0.26, NNZs: 30, Bias: 0.115000, T: 85129, Avg. loss: 0.006839\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 0.26, NNZs: 30, Bias: 0.125000, T: 92868, Avg. loss: 0.007096\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 0.22, NNZs: 30, Bias: 0.125000, T: 100607, Avg. loss: 0.007044\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 14\n",
      "Norm: 0.24, NNZs: 30, Bias: 0.120000, T: 108346, Avg. loss: 0.006916\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 15\n",
      "Norm: 0.23, NNZs: 30, Bias: 0.125000, T: 116085, Avg. loss: 0.006967\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 16\n",
      "Norm: 0.26, NNZs: 30, Bias: 0.120000, T: 123824, Avg. loss: 0.006773\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 17\n",
      "Norm: 0.25, NNZs: 30, Bias: 0.140000, T: 131563, Avg. loss: 0.007212\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 18\n",
      "Norm: 0.27, NNZs: 30, Bias: 0.115000, T: 139302, Avg. loss: 0.006890\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 19\n",
      "Norm: 0.25, NNZs: 30, Bias: 0.115000, T: 147041, Avg. loss: 0.007097\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 20\n",
      "Norm: 0.26, NNZs: 30, Bias: 0.120000, T: 154780, Avg. loss: 0.006938\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 21\n",
      "Norm: 0.27, NNZs: 30, Bias: 0.115000, T: 162519, Avg. loss: 0.007028\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 22\n",
      "Norm: 0.25, NNZs: 30, Bias: 0.125000, T: 170258, Avg. loss: 0.007097\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 23\n",
      "Norm: 0.23, NNZs: 30, Bias: 0.125000, T: 177997, Avg. loss: 0.007147\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 24\n",
      "Norm: 0.26, NNZs: 30, Bias: 0.125000, T: 185736, Avg. loss: 0.006965\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 25\n",
      "Norm: 0.25, NNZs: 30, Bias: 0.130000, T: 193475, Avg. loss: 0.007126\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 26\n",
      "Norm: 0.26, NNZs: 30, Bias: 0.115000, T: 201214, Avg. loss: 0.007362\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 27\n",
      "Norm: 0.25, NNZs: 30, Bias: 0.130000, T: 208953, Avg. loss: 0.006766\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 28\n",
      "Norm: 0.24, NNZs: 30, Bias: 0.125000, T: 216692, Avg. loss: 0.007087\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 29\n",
      "Norm: 0.24, NNZs: 30, Bias: 0.140000, T: 224431, Avg. loss: 0.007029\n",
      "Total training time: 0.07 seconds.\n",
      "-- Epoch 30\n",
      "Norm: 0.25, NNZs: 30, Bias: 0.120000, T: 232170, Avg. loss: 0.007092\n",
      "Total training time: 0.07 seconds.\n",
      "-- Epoch 31\n",
      "Norm: 0.26, NNZs: 30, Bias: 0.115000, T: 239909, Avg. loss: 0.006791\n",
      "Total training time: 0.07 seconds.\n",
      "-- Epoch 32\n",
      "Norm: 0.26, NNZs: 30, Bias: 0.145000, T: 247648, Avg. loss: 0.007113\n",
      "Total training time: 0.07 seconds.\n",
      "-- Epoch 33\n",
      "Norm: 0.25, NNZs: 30, Bias: 0.135000, T: 255387, Avg. loss: 0.006971\n",
      "Total training time: 0.07 seconds.\n",
      "-- Epoch 34\n",
      "Norm: 0.26, NNZs: 30, Bias: 0.120000, T: 263126, Avg. loss: 0.006985\n",
      "Total training time: 0.08 seconds.\n",
      "-- Epoch 35\n",
      "Norm: 0.26, NNZs: 30, Bias: 0.120000, T: 270865, Avg. loss: 0.007053\n",
      "Total training time: 0.08 seconds.\n",
      "-- Epoch 36\n",
      "Norm: 0.26, NNZs: 30, Bias: 0.125000, T: 278604, Avg. loss: 0.007124\n",
      "Total training time: 0.08 seconds.\n",
      "-- Epoch 37\n",
      "Norm: 0.28, NNZs: 30, Bias: 0.125000, T: 286343, Avg. loss: 0.006837\n",
      "Total training time: 0.08 seconds.\n",
      "-- Epoch 38\n",
      "Norm: 0.27, NNZs: 30, Bias: 0.125000, T: 294082, Avg. loss: 0.007179\n",
      "Total training time: 0.08 seconds.\n",
      "-- Epoch 39\n",
      "Norm: 0.26, NNZs: 30, Bias: 0.115000, T: 301821, Avg. loss: 0.007251\n",
      "Total training time: 0.08 seconds.\n",
      "-- Epoch 40\n",
      "Norm: 0.25, NNZs: 30, Bias: 0.125000, T: 309560, Avg. loss: 0.007024\n",
      "Total training time: 0.09 seconds.\n",
      "-- Epoch 41\n",
      "Norm: 0.24, NNZs: 30, Bias: 0.130000, T: 317299, Avg. loss: 0.006943\n",
      "Total training time: 0.09 seconds.\n",
      "-- Epoch 42\n",
      "Norm: 0.25, NNZs: 30, Bias: 0.125000, T: 325038, Avg. loss: 0.006737\n",
      "Total training time: 0.09 seconds.\n",
      "-- Epoch 43\n",
      "Norm: 0.24, NNZs: 30, Bias: 0.125000, T: 332777, Avg. loss: 0.007264\n",
      "Total training time: 0.09 seconds.\n",
      "-- Epoch 44\n",
      "Norm: 0.25, NNZs: 30, Bias: 0.140000, T: 340516, Avg. loss: 0.007173\n",
      "Total training time: 0.10 seconds.\n",
      "-- Epoch 45\n",
      "Norm: 0.25, NNZs: 30, Bias: 0.135000, T: 348255, Avg. loss: 0.006564\n",
      "Total training time: 0.11 seconds.\n",
      "-- Epoch 46\n",
      "Norm: 0.25, NNZs: 30, Bias: 0.135000, T: 355994, Avg. loss: 0.006912\n",
      "Total training time: 0.11 seconds.\n",
      "-- Epoch 47\n",
      "Norm: 0.26, NNZs: 30, Bias: 0.125000, T: 363733, Avg. loss: 0.007058\n",
      "Total training time: 0.12 seconds.\n",
      "-- Epoch 48\n",
      "Norm: 0.26, NNZs: 30, Bias: 0.135000, T: 371472, Avg. loss: 0.007136\n",
      "Total training time: 0.12 seconds.\n",
      "-- Epoch 49\n",
      "Norm: 0.27, NNZs: 30, Bias: 0.115000, T: 379211, Avg. loss: 0.006905\n",
      "Total training time: 0.12 seconds.\n",
      "-- Epoch 50\n",
      "Norm: 0.26, NNZs: 30, Bias: 0.140000, T: 386950, Avg. loss: 0.007165\n",
      "Total training time: 0.12 seconds.\n",
      "-- Epoch 51\n",
      "Norm: 0.24, NNZs: 30, Bias: 0.130000, T: 394689, Avg. loss: 0.006787\n",
      "Total training time: 0.12 seconds.\n",
      "-- Epoch 52\n",
      "Norm: 0.24, NNZs: 30, Bias: 0.155000, T: 402428, Avg. loss: 0.007252\n",
      "Total training time: 0.13 seconds.\n",
      "-- Epoch 53\n",
      "Norm: 0.27, NNZs: 30, Bias: 0.130000, T: 410167, Avg. loss: 0.006974\n",
      "Total training time: 0.13 seconds.\n",
      "-- Epoch 54\n",
      "Norm: 0.26, NNZs: 30, Bias: 0.140000, T: 417906, Avg. loss: 0.007045\n",
      "Total training time: 0.13 seconds.\n",
      "-- Epoch 55\n",
      "Norm: 0.24, NNZs: 30, Bias: 0.150000, T: 425645, Avg. loss: 0.007025\n",
      "Total training time: 0.13 seconds.\n",
      "-- Epoch 56\n",
      "Norm: 0.27, NNZs: 30, Bias: 0.140000, T: 433384, Avg. loss: 0.006713\n",
      "Total training time: 0.14 seconds.\n",
      "-- Epoch 57\n",
      "Norm: 0.24, NNZs: 30, Bias: 0.160000, T: 441123, Avg. loss: 0.007258\n",
      "Total training time: 0.14 seconds.\n",
      "-- Epoch 58\n",
      "Norm: 0.26, NNZs: 30, Bias: 0.150000, T: 448862, Avg. loss: 0.007309\n",
      "Total training time: 0.14 seconds.\n",
      "-- Epoch 59\n",
      "Norm: 0.24, NNZs: 30, Bias: 0.145000, T: 456601, Avg. loss: 0.007151\n",
      "Total training time: 0.14 seconds.\n",
      "-- Epoch 60\n",
      "Norm: 0.27, NNZs: 30, Bias: 0.150000, T: 464340, Avg. loss: 0.007316\n",
      "Total training time: 0.15 seconds.\n",
      "-- Epoch 61\n",
      "Norm: 0.27, NNZs: 30, Bias: 0.145000, T: 472079, Avg. loss: 0.007068\n",
      "Total training time: 0.15 seconds.\n",
      "-- Epoch 62\n",
      "Norm: 0.25, NNZs: 30, Bias: 0.150000, T: 479818, Avg. loss: 0.007009\n",
      "Total training time: 0.15 seconds.\n",
      "-- Epoch 63\n",
      "Norm: 0.27, NNZs: 30, Bias: 0.150000, T: 487557, Avg. loss: 0.007151\n",
      "Total training time: 0.15 seconds.\n",
      "-- Epoch 64\n",
      "Norm: 0.25, NNZs: 30, Bias: 0.140000, T: 495296, Avg. loss: 0.007134\n",
      "Total training time: 0.16 seconds.\n",
      "-- Epoch 65\n",
      "Norm: 0.26, NNZs: 30, Bias: 0.140000, T: 503035, Avg. loss: 0.006871\n",
      "Total training time: 0.16 seconds.\n",
      "-- Epoch 66\n",
      "Norm: 0.27, NNZs: 30, Bias: 0.130000, T: 510774, Avg. loss: 0.006795\n",
      "Total training time: 0.16 seconds.\n",
      "-- Epoch 67\n",
      "Norm: 0.26, NNZs: 30, Bias: 0.125000, T: 518513, Avg. loss: 0.006715\n",
      "Total training time: 0.16 seconds.\n",
      "-- Epoch 68\n",
      "Norm: 0.27, NNZs: 30, Bias: 0.135000, T: 526252, Avg. loss: 0.007151\n",
      "Total training time: 0.17 seconds.\n",
      "-- Epoch 69\n",
      "Norm: 0.24, NNZs: 30, Bias: 0.160000, T: 533991, Avg. loss: 0.006989\n",
      "Total training time: 0.17 seconds.\n",
      "-- Epoch 70\n",
      "Norm: 0.27, NNZs: 30, Bias: 0.150000, T: 541730, Avg. loss: 0.007082\n",
      "Total training time: 0.18 seconds.\n",
      "-- Epoch 71\n",
      "Norm: 0.26, NNZs: 30, Bias: 0.145000, T: 549469, Avg. loss: 0.007285\n",
      "Total training time: 0.18 seconds.\n",
      "-- Epoch 72\n",
      "Norm: 0.27, NNZs: 30, Bias: 0.150000, T: 557208, Avg. loss: 0.006654\n",
      "Total training time: 0.18 seconds.\n",
      "-- Epoch 73\n",
      "Norm: 0.27, NNZs: 30, Bias: 0.155000, T: 564947, Avg. loss: 0.006835\n",
      "Total training time: 0.19 seconds.\n",
      "-- Epoch 74\n",
      "Norm: 0.26, NNZs: 30, Bias: 0.155000, T: 572686, Avg. loss: 0.007062\n",
      "Total training time: 0.19 seconds.\n",
      "-- Epoch 75\n",
      "Norm: 0.26, NNZs: 30, Bias: 0.145000, T: 580425, Avg. loss: 0.007137\n",
      "Total training time: 0.19 seconds.\n",
      "-- Epoch 76\n",
      "Norm: 0.25, NNZs: 30, Bias: 0.140000, T: 588164, Avg. loss: 0.006795\n",
      "Total training time: 0.20 seconds.\n",
      "-- Epoch 77\n",
      "Norm: 0.26, NNZs: 30, Bias: 0.145000, T: 595903, Avg. loss: 0.006999\n",
      "Total training time: 0.20 seconds.\n",
      "-- Epoch 78\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norm: 0.25, NNZs: 30, Bias: 0.155000, T: 603642, Avg. loss: 0.006911\n",
      "Total training time: 0.20 seconds.\n",
      "-- Epoch 79\n",
      "Norm: 0.26, NNZs: 30, Bias: 0.155000, T: 611381, Avg. loss: 0.006847\n",
      "Total training time: 0.22 seconds.\n",
      "-- Epoch 80\n",
      "Norm: 0.27, NNZs: 30, Bias: 0.150000, T: 619120, Avg. loss: 0.007071\n",
      "Total training time: 0.22 seconds.\n",
      "-- Epoch 81\n",
      "Norm: 0.27, NNZs: 30, Bias: 0.160000, T: 626859, Avg. loss: 0.007067\n",
      "Total training time: 0.23 seconds.\n",
      "-- Epoch 82\n",
      "Norm: 0.26, NNZs: 30, Bias: 0.155000, T: 634598, Avg. loss: 0.007305\n",
      "Total training time: 0.23 seconds.\n",
      "-- Epoch 83\n",
      "Norm: 0.28, NNZs: 30, Bias: 0.160000, T: 642337, Avg. loss: 0.006934\n",
      "Total training time: 0.23 seconds.\n",
      "-- Epoch 84\n",
      "Norm: 0.25, NNZs: 30, Bias: 0.180000, T: 650076, Avg. loss: 0.007028\n",
      "Total training time: 0.24 seconds.\n",
      "-- Epoch 85\n",
      "Norm: 0.26, NNZs: 30, Bias: 0.155000, T: 657815, Avg. loss: 0.007067\n",
      "Total training time: 0.24 seconds.\n",
      "-- Epoch 86\n",
      "Norm: 0.26, NNZs: 30, Bias: 0.160000, T: 665554, Avg. loss: 0.007132\n",
      "Total training time: 0.25 seconds.\n",
      "-- Epoch 87\n",
      "Norm: 0.27, NNZs: 30, Bias: 0.155000, T: 673293, Avg. loss: 0.006777\n",
      "Total training time: 0.25 seconds.\n",
      "-- Epoch 88\n",
      "Norm: 0.26, NNZs: 30, Bias: 0.160000, T: 681032, Avg. loss: 0.006843\n",
      "Total training time: 0.25 seconds.\n",
      "-- Epoch 89\n",
      "Norm: 0.27, NNZs: 30, Bias: 0.160000, T: 688771, Avg. loss: 0.006901\n",
      "Total training time: 0.26 seconds.\n",
      "-- Epoch 90\n",
      "Norm: 0.27, NNZs: 30, Bias: 0.145000, T: 696510, Avg. loss: 0.007182\n",
      "Total training time: 0.26 seconds.\n",
      "-- Epoch 91\n",
      "Norm: 0.27, NNZs: 30, Bias: 0.145000, T: 704249, Avg. loss: 0.006713\n",
      "Total training time: 0.26 seconds.\n",
      "-- Epoch 92\n",
      "Norm: 0.26, NNZs: 30, Bias: 0.155000, T: 711988, Avg. loss: 0.007102\n",
      "Total training time: 0.26 seconds.\n",
      "-- Epoch 93\n",
      "Norm: 0.27, NNZs: 30, Bias: 0.170000, T: 719727, Avg. loss: 0.007106\n",
      "Total training time: 0.26 seconds.\n",
      "-- Epoch 94\n",
      "Norm: 0.26, NNZs: 30, Bias: 0.165000, T: 727466, Avg. loss: 0.007187\n",
      "Total training time: 0.26 seconds.\n",
      "-- Epoch 95\n",
      "Norm: 0.26, NNZs: 30, Bias: 0.165000, T: 735205, Avg. loss: 0.007004\n",
      "Total training time: 0.27 seconds.\n",
      "-- Epoch 96\n",
      "Norm: 0.27, NNZs: 30, Bias: 0.145000, T: 742944, Avg. loss: 0.006996\n",
      "Total training time: 0.27 seconds.\n",
      "-- Epoch 97\n",
      "Norm: 0.27, NNZs: 30, Bias: 0.145000, T: 750683, Avg. loss: 0.007265\n",
      "Total training time: 0.27 seconds.\n",
      "-- Epoch 98\n",
      "Norm: 0.28, NNZs: 30, Bias: 0.155000, T: 758422, Avg. loss: 0.007100\n",
      "Total training time: 0.27 seconds.\n",
      "-- Epoch 99\n",
      "Norm: 0.25, NNZs: 30, Bias: 0.165000, T: 766161, Avg. loss: 0.006992\n",
      "Total training time: 0.27 seconds.\n",
      "-- Epoch 100\n",
      "Norm: 0.26, NNZs: 30, Bias: 0.160000, T: 773900, Avg. loss: 0.006870\n",
      "Total training time: 0.27 seconds.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Perceptron(alpha=0.0001, class_weight=None, eta0=0.005, fit_intercept=True,\n",
       "      max_iter=100, n_iter=None, n_jobs=1, penalty=None, random_state=None,\n",
       "      shuffle=True, tol=None, verbose=1, warm_start=False)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the perceptron object (net)\n",
    "net = perceptron.Perceptron(max_iter=100, verbose=1, random_state=None, fit_intercept=True, eta0=0.005)\n",
    "\n",
    "inputs_train_list = inputs_train[['having_IP_Address','URL_Length','Shortining_Service','having_At_Symbol','double_slash_redirecting','Prefix_Suffix','having_Sub_Domain','SSLfinal_State','Domain_registeration_length','Favicon','port','HTTPS_token','Request_URL','URL_of_Anchor','Links_in_tags','SFH','Submitting_to_email','Abnormal_URL','Redirect','on_mouseover','RightClick','popUpWidnow','Iframe','age_of_domain','DNSRecord','web_traffic','Page_Rank','Google_Index','Links_pointing_to_page','Statistical_report']]\n",
    "# Train the perceptron object (net)\n",
    "net.fit(inputs_train_list,inputs_train['Result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficient 0 0.03\n",
      "Coefficient 1 -0.015\n",
      "Bias [ 0.16]\n"
     ]
    }
   ],
   "source": [
    "# Output the coefficints\n",
    "print(\"Coefficient 0 \" + str(net.coef_[0,0]))\n",
    "print(\"Coefficient 1 \" + str(net.coef_[0,1])) \n",
    "print(\"Bias \" + str(net.intercept_)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy of Perceptron Model: 81.724970%\n"
     ]
    }
   ],
   "source": [
    "# Do a prediction\n",
    "\n",
    "inputs_test_list = inputs_test[['having_IP_Address','URL_Length','Shortining_Service','having_At_Symbol','double_slash_redirecting','Prefix_Suffix','having_Sub_Domain','SSLfinal_State','Domain_registeration_length','Favicon','port','HTTPS_token','Request_URL','URL_of_Anchor','Links_in_tags','SFH','Submitting_to_email','Abnormal_URL','Redirect','on_mouseover','RightClick','popUpWidnow','Iframe','age_of_domain','DNSRecord','web_traffic','Page_Rank','Google_Index','Links_pointing_to_page','Statistical_report']]\n",
    "\n",
    "pred = net.predict(inputs_test_list)\n",
    "\n",
    "# Confusion Matrix\n",
    "# confusion_matrix(pred, inputs['Result'])\n",
    "print(\"accuracy of Perceptron Model: {0:2f}%\".format(accuracy_score(inputs_test[['Result']], pred) *100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.52416247\n",
      "Iteration 2, loss = 0.29927320\n",
      "Iteration 3, loss = 0.22430397\n",
      "Iteration 4, loss = 0.19442258\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/sklearn/neural_network/multilayer_perceptron.py:912: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 5, loss = 0.17949677\n",
      "Iteration 6, loss = 0.16938982\n",
      "Iteration 7, loss = 0.16319937\n",
      "Iteration 8, loss = 0.15652121\n",
      "Iteration 9, loss = 0.15169963\n",
      "Iteration 10, loss = 0.14867623\n",
      "Iteration 11, loss = 0.14423667\n",
      "Iteration 12, loss = 0.14084982\n",
      "Iteration 13, loss = 0.13923286\n",
      "Iteration 14, loss = 0.13604026\n",
      "Iteration 15, loss = 0.13359956\n",
      "Iteration 16, loss = 0.13140882\n",
      "Iteration 17, loss = 0.12920495\n",
      "Iteration 18, loss = 0.12528534\n",
      "Iteration 19, loss = 0.12380790\n",
      "Iteration 20, loss = 0.12144733\n",
      "Iteration 21, loss = 0.11955943\n",
      "Iteration 22, loss = 0.11823848\n",
      "Iteration 23, loss = 0.11468805\n",
      "Iteration 24, loss = 0.11376613\n",
      "Iteration 25, loss = 0.11159483\n",
      "Iteration 26, loss = 0.10898873\n",
      "Iteration 27, loss = 0.10749269\n",
      "Iteration 28, loss = 0.10615011\n",
      "Iteration 29, loss = 0.10461756\n",
      "Iteration 30, loss = 0.10293769\n",
      "Iteration 31, loss = 0.10188999\n",
      "Iteration 32, loss = 0.10016734\n",
      "Iteration 33, loss = 0.09840693\n",
      "Iteration 34, loss = 0.09702197\n",
      "Iteration 35, loss = 0.09503207\n",
      "Iteration 36, loss = 0.09665433\n",
      "Iteration 37, loss = 0.09413767\n",
      "Iteration 38, loss = 0.09123067\n",
      "Iteration 39, loss = 0.09146881\n",
      "Iteration 40, loss = 0.08875970\n",
      "Iteration 41, loss = 0.08776859\n",
      "Iteration 42, loss = 0.08892360\n",
      "Iteration 43, loss = 0.08806140\n",
      "Iteration 44, loss = 0.08704318\n",
      "Iteration 45, loss = 0.08758469\n",
      "Iteration 46, loss = 0.08261720\n",
      "Iteration 47, loss = 0.08260905\n",
      "Iteration 48, loss = 0.08104567\n",
      "Iteration 49, loss = 0.07999932\n",
      "Iteration 50, loss = 0.07945745\n",
      "Iteration 51, loss = 0.07811646\n",
      "Iteration 52, loss = 0.07823086\n",
      "Iteration 53, loss = 0.07624185\n",
      "Iteration 54, loss = 0.07537519\n",
      "Iteration 55, loss = 0.07369895\n",
      "Iteration 56, loss = 0.07364121\n",
      "Iteration 57, loss = 0.07467518\n",
      "Iteration 58, loss = 0.07369498\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
       "       nesterovs_momentum=True, power_t=0.5, random_state=None,\n",
       "       shuffle=True, solver='adam', tol=0.0001, validation_fraction=0.1,\n",
       "       verbose=True, warm_start=False)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp = MLPClassifier(verbose=True)\n",
    "\n",
    "mlp.fit(inputs_train_list,inputs_train[['Result']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "         -1       0.93      0.84      0.88      1463\n",
      "          1       0.88      0.95      0.92      1853\n",
      "\n",
      "avg / total       0.90      0.90      0.90      3316\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions = mlp.predict(inputs_test_list)\n",
    "print(classification_report(inputs_test[['Result']],predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
